{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chaospy\n",
    "import numpoly\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"bmh\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn, twkm_orig=289.65):\n",
    "    df = pd.read_csv(fn, index_col=0, header=[0,1,2,3,4]).T\n",
    "    df[\"offwind\"] = df[\"offwind-ac\"] + df[\"offwind-dc\"]\n",
    "    df[\"wind\"] = df[\"onwind\"] + df[\"offwind\"]\n",
    "    df[\"transmission\"] = df[\"lines\"] + df[\"links\"] + twkm_orig\n",
    "    df.drop([\"ror\", \"hydro\", 'PHS', 'offwind-ac', 'offwind-dc', 'lines', 'links'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiindex2df(multiindex):\n",
    "    return pd.DataFrame(multiindex2array(multiindex), index=multiindex.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiindex2array(multiindex):\n",
    "    return np.array([np.array(row).astype(float) for row in multiindex]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedJ:\n",
    "    \"\"\"Dictionary-like wrapper for joint random variable generator.\"\"\"\n",
    "\n",
    "    def __init__(self, distributions):\n",
    "        self.J = self.J_from_dict(distributions.values())  \n",
    "        self.names = distributions.keys()\n",
    "        self.mapping = {k: i for i, k in enumerate(self.names)}\n",
    "        \n",
    "    def __getitem__(self, attr):\n",
    "        return self.J[self.mapping[attr]]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([f\"{k}: {self[k]}\" for k in self.names])\n",
    "    \n",
    "    def J_from_dict(self, values):\n",
    "        DD = []\n",
    "        for v in values:\n",
    "            D = getattr(chaospy, v[\"type\"])\n",
    "            DD.append(D(*v[\"args\"]))\n",
    "        return chaospy.J(*DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedPoly:\n",
    "    \"\"\"Dictionary-like wrapper for vector numpoly polynomials.\"\"\"\n",
    "    \n",
    "    def __init__(self, poly, names):\n",
    "        self.poly = poly\n",
    "        self.names = list(names)\n",
    "        self.mapping = {k: i for i, k in enumerate(self.names)}\n",
    "        \n",
    "    def __getitem__(self, attr):\n",
    "        return self.poly[self.mapping[attr]]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        r = numpoly.array_repr\n",
    "        return \"\\n\\n\".join([f\"{k}: {r(self.poly[i])}\" for i, k in enumerate(self.names)]) \n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        return pd.Series(self.poly(*args), index=self.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_surrogate(order, distribution, training_set, model=None):\n",
    "    samples = multiindex2array(training_set.index)\n",
    "    pce = chaospy.orth_ttr(order, distribution.J)\n",
    "    surrogate = chaospy.fit_regression(pce, samples, training_set.values, model)\n",
    "    variables = training_set.columns\n",
    "    return NamedPoly(surrogate, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction(surrogate, samples):\n",
    "    prediction = samples.apply(lambda s: surrogate(*s), result_type='expand')\n",
    "    prediction.columns = pd.MultiIndex.from_frame(samples.astype(str).T)\n",
    "    return prediction.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(truth, predictions, fn=None):\n",
    "    \n",
    "    if not isinstance(predictions, list):\n",
    "        predictions = [predictions]\n",
    "    \n",
    "    fig, axes = plt.subplots(2,4,figsize=(10,5))\n",
    "    for i, c in enumerate(truth.columns):\n",
    "        ax = axes[int(i/4)][i%4]\n",
    "        ax.set_title(c)\n",
    "        ax.set_ylim([0,0.1])\n",
    "        bins = np.arange(-70,71,5)\n",
    "        if c == \"tsc\":\n",
    "            ax.set_xlim([-2.5,2.5])\n",
    "            ax.set_ylim([0,0.7])\n",
    "            ax.set_xlabel(\"bn EUR p.a.\")\n",
    "            bins = np.arange(-2.5,2.6,0.25)\n",
    "        elif c == \"transmission\":\n",
    "            ax.set_xlim([-75,75])\n",
    "            ax.set_xlabel(\"TWkm\")\n",
    "        else:\n",
    "            ax.set_xlim([-75,75])\n",
    "            ax.set_xlabel(\"GW\")\n",
    "        for j, p in enumerate(predictions):\n",
    "            (p - truth)[c].plot.hist(ax=ax, label=f\"pred. {j}\", alpha=0.4, bins=bins, density=True)\n",
    "    plt.tight_layout()\n",
    "    axes[1,0].legend();\n",
    "    if fn is not None:\n",
    "        plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(prediction, truth, fn=None):\n",
    "    fig, ax = plt.subplots(figsize=(30,5))\n",
    "    sns.heatmap((prediction - truth).T, cmap='vlag', vmin=-50, vmax=50)\n",
    "    if fn is not None:\n",
    "        plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sobol(sobol, fn=None):\n",
    "    fig, ax = plt.subplots(figsize=(5,8))\n",
    "    sns.heatmap(sobol, square=True, cmap=\"Blues\", vmax=1, vmin=0, annot=True, fmt=\".2f\", cbar=False);\n",
    "    if fn is not None:\n",
    "        plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(prediction, truth):\n",
    "    kws = dict(multioutput=\"raw_values\")\n",
    "    diff = prediction - truth\n",
    "    return pd.concat({\n",
    "        \"mape\": diff.abs().mean() / truth.mean() * 100,\n",
    "        \"mae\": diff.abs().mean(),\n",
    "        \"r2\": pd.Series(r2_score(test_set, test_predictions, **kws), index=truth.columns),\n",
    "        \"variance_explained\": pd.Series(explained_variance_score(truth, prediction, **kws), index=truth.columns)\n",
    "    }, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sobol(surrogate, distribution, decimals=3, total=True):\n",
    "    func = chaospy.Sens_t if total else chaospy.Sens_m\n",
    "    sobol = func(surrogate.poly, distribution.J).round(decimals)\n",
    "    return pd.DataFrame(sobol, index=distribution.names, columns=surrogate.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"../results/capacities-50halton.csv\"\n",
    "fn_surrogate = \"surrogate.txt\"\n",
    "order = 3\n",
    "# scale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data(datafile)\n",
    "distribution = NamedJ(config[\"uncertainties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(dataset, train_size=180, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on polynomial regression == linear regression\n",
    "# https://scikit-learn.org/stable/modules/linear_model.html\n",
    "#model = lm.Lars(fit_intercept=False)\n",
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = build_surrogate(order, distribution, train_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpoly.savetxt(fn_surrogate, surrogate.poly, header=\" \".join(surrogate.names), fmt=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = multiindex2df(train_set.index)\n",
    "train_predictions = build_prediction(surrogate, train_samples)\n",
    "\n",
    "test_samples = multiindex2df(test_set.index)\n",
    "test_predictions = build_prediction(surrogate, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# optional\n",
    "if scale:\n",
    "    scaler = StandardScaler().fit(train_set)\n",
    "\n",
    "    def transform(df, scaler):\n",
    "        return pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "    def inverse_transform(df, scaler):\n",
    "        return pd.DataFrame(scaler.inverse_transform(df), index=df.index, columns=df.columns)\n",
    "\n",
    "    train_set = transform(train_set, scaler)\n",
    "    test_set = transform(test_set, scaler)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# optional\n",
    "if scale:\n",
    "    train_predictions = inverse_transform(train_predictions, scaler)\n",
    "    test_predictions = inverse_transform(test_predictions, scaler)\n",
    "    train_set = inverse_transform(train_set, scaler)\n",
    "    test_set = inverse_transform(test_set, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(dataset, [train_predictions, test_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.mean() - test_set.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_errors(train_predictions, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_errors(test_predictions, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sobol(surrogate, distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = numpoly.loadtxt(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-fidelity approach\n",
    "\n",
    "- many more samples in very low resolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Machine Learning with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neural_network as ann\n",
    "model = ann.MLPRegressor(max_iter=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_samples.T, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_ml(model, train_samples, mirror):\n",
    "    prediction = model.predict(train_samples.T)\n",
    "    return pd.DataFrame(prediction, index=mirror.index, columns=mirror.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = build_prediction_ml(model, train_samples, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = build_prediction_ml(model, test_samples, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_errors(train_predictions, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_errors(test_predictions, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histograms(dataset, [train_predictions, test_predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Easy Benchmark to Beat:\n",
    "\n",
    "Surrogate is obtained from MC sampling for 37 nodes and 6-hourly resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}