\subsection{Power System Investment Planning}
\label{sec:model}
%\blindtext

\subsubsection{Least-Cost Optimisation}

%objective
The objective is to minimise the total annual system costs comprising generation, transmission and storage infrastructure in a fully renewable system. Stylised as $c^\top x$.

% constraints
The objective is subject to linear constraints $Ax=b$ that define limits on
(i) the capacities of infrastructure from geographical and technical potentials,
(ii) the availability of variable renewable energy sources for each location and point in time derived from reanalysis weather data, and
(iii) linearised multi-period optimal power flow (LOPF) constraints including storage consistency equations.

\begin{equation}
    C = \min_x\{c^\top x \mid Ax\leq b\}
\end{equation}

\subsubsection{Near-Optimal Solutions}

Following the methodology presented in \cite{nearoptimal}
epsilon constraint method from multi-objective optimisation \cite{mavrotas_effective_2009}

one technology

- new objective
- add constraint to limit cost increase

$x_s\subseteq x$ (for instance solar capacities)

\begin{align}
    \overline{x_s} = \max_{x_s}\{\:&x_s \mid Ax\leq b,\quad c^\top x\leq (1+\varepsilon)\cdot C \:\} \\
\end{align}

for generation (any wind, onshore wind, offshore wind, solar)
for transmission
for storage (hydrogen, battery)

two technologies

- add constraint to fix total capacity

$x_w\subseteq x$ (for instance wind capacities)

\begin{equation}
    \overline{x_w} = \max_{x_w}\{\:x_w \mid Ax\leq b,\quad c^\top x\leq (1+\varepsilon)\cdot C, \quad x_s = \underline{x_s} + \alpha \cdot (\overline{x_s}-\underline{x_s}) \:\}
\end{equation}

for pairs
(i) wind and solar,
(ii) offshore and onshore wind,
(iii) hydrogen and battery storage

more sophisticated than discrete steps to map the 2D crossection in \cite{pedersen_modeling_2020}


\subsubsection{Power System Model}

%F map of 37 and 128 nodes and time series 4H/2H at particular location

\begin{SCfigure}
    \begin{tabular}{cc}
        \footnotesize low-fidelity: 37 nodes and 4-hourly & \footnotesize high-fidelity: 128 nodes and 2-hourly \\
        \includegraphics[width=0.33\textwidth]{map37.pdf} &
        \includegraphics[width=0.33\textwidth]{map128.pdf} \\
        \includegraphics[width=0.33\textwidth]{timeseries37.pdf} &
        \includegraphics[width=0.33\textwidth]{timeseries128.pdf} \\
    \end{tabular}
    \caption{Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.}
\end{SCfigure}

PyPSA-Eur \cite{pypsa} \cite{pypsaeur} \cite{hoersch_spatial_2017} 

open-source model built as snakemake workflow \cite{snakemake}

only power system

100\% renewable system based on wind and solar

greenfield, exception: today's transmission grid, hydro, run-of-river


\subsection{Technology Cost Uncertainty}
\label{sec:uncertainty}
%\blindtext

"the uncertainties inherent in the model structures and input parameters are at best underplayed and at worst ignored" \cite{yue_review_2018}
it is more insightful than using best-guess values \cite{fraiture_robustness_2020}
but quantifying the input uncertainties is a challenge \cite{moret_characterization_2017}

Two main sources of technology cost uncertainty \cite{trondle_trade-offs_2020}
- future deployment rates unknown
- learning rate unknown
- models "highly sensitive to uncertainty in the learning rates [...] due to the exponential relationship" \cite{mattsson_learning_2019}
- \cite{yeh_review_2012}
- \cite{heuberger_power_2017}
- \cite{gritsevskyi_modeling_2000}
- \cite{schmidt_projecting_2019}
- \cite{schmidt_future_2017}

\subsubsection{Ranges}

\begin{SCtable}
    \begin{small}
        \begin{tabular}{cccc}
            \toprule
            Technology & Lower Annuity & Upper Annuity & Unit  \\ \midrule
            Onshore Wind & 73 & 109 & EUR/kW/a \\
            Offshore Wind & 178 & 245 & EUR/kW/a \\ % this includes connection cost!
            Solar & 36 & 53 & EUR/kW/a \\
            Battery & 30 & 125 & EUR/kW/a \\
            Hydrogen & 111 & 259 & EUR/kW/a \\ \bottomrule
        \end{tabular}
    \end{small}
    \caption{Technology cost uncertainty using optimistic and pessimistic assumptions from DEA.}
    \label{tab:costuncertainty}
\end{SCtable}   

vital to choose widest plausible parameter range and not exclude plausible scenarios \cite{moret_characterization_2017,mccollum_energy_2020}

Danish Energy Agency source
- continuously updated since 2016
- take optimistic and pessimistic values
- correspond to a 90\% confidence interval
- for 2050, but supplemented with 2030 values if widen range
- no ranges rooftop PV
- `./costcomparison.csv`

Comparison to others:
- \cite{trondle_trade-offs_2020} has almost always more conservative values than pessmistic DEA database; data mostly from ETIP
- previous studies have considered "relatively narrow range[s] of uncertainties" \cite{Li2017}
- $\pm$20\% \cite{moret_characterization_2017}
- mostly less than $\pm$25\% \cite{pizarro-alonso_uncertainties_2019}
- $\pm$50\% \cite{shirizadeh_how_2019}

\subsubsection{Distributions}

distributions of cost projections
- uniform: \cite{moret_characterization_2017,moret_robust_2016,shirizadeh_how_2019,trondle_trade-offs_2020,pilpola_analyzing_2020,Li2017,Trutnevyte2013,lopion_cost_2019}
- normal: \cite{mavromatidis_uncertainty_2018}
- triangle: \cite{li_using_2020}
- variance set depending on maturity of a technology \cite{li_using_2020}
- difficult to sensibly assign a probability distribution to cost uncertainties \cite{moret_robust_2016}

Just as \cite{trondle_trade-offs_2020} we pursue a maximum entropy approach. 
- This means we assume the costs are uniformly distributed between the DEA cost ranges and independent.
- always independently sampled (IID); easy but incorrect: e.g. synergies between offshore and onshore wind
- Given the lack of knowledge about the distribution, the uniform distribution makes fewest assumptions.

\subsection{Surrogate Modelling}
\label{sec:surrogate}
%\blindtext

premise: outcome of original model cannot be obtained easily due to computational constraints
synonyms for surrogate: emulators, approximation models, response surface methods, metamodels
consider only simplified/aggregated outputs
same input/output behaviour as original model, but faster to compute (link to machine learning)
use cases: design space exploration, sensitivity analysis, what-if analyis
"reduce the number of deterministic evaluations while retaining accuracy" \cite{palar_multi-fidelity_2016}
one method is building a surrogate model based on polynomial chaos expansion

% this paper
used inputs: cost of offshore, onshore, solar, H2, battery
recorded outputs: system cost, wind, offwind, onwind, solar, H2, battery, transmission

\subsubsection{Polynomial Chaos Expansion}

detailed general info and advise
\cite{gratiet_metamodel-based_2015} 
\cite{sudret_global_2008}
\cite{fajraoui_optimal_2017}
\cite{marelli_uqlab_nodate}

Use PCE to build surrogate models \cite{sudret_global_2008}
- Hilbert space technique 
- Hilbert space is like Euclidian space for polynomials, polynomials are so to speak the coordinates \cite{gratiet_metamodel-based_2015})
- polynomial expansion is a collection of polynomials
- The analogy for a polynomial chaos expansion of a random variable in signal processing is a Fourier series \cite{muhlpfordt_uncertainty_2020}
- also related to PCA
- needs finite variance!
- idea: "expand random variables as a linear combination of orthogonal basis functions weighted by deterministic coefficients." \cite{muhlpfordt_uncertainty_2020}
- techniques for finding coefficients:
  - spectral projection: project response onto basis functions using inner products and orthogonal polynomials \cite{palar_multi-fidelity_2016}
  - point collocation: obtain coefficients by performing regression \cite{palar_multi-fidelity_2016,fajraoui_optimal_2017}

% in this paper
- want to do non-intrusive (i.e. wrapping around model) point collocation (i.e. MC sampling) based on regression \cite{ng_multifidelity_2012}
- one surrogate model for each $\varepsilon$ (5), min/max optimisation sense (2), objective (7) and output variable (8).

% general procedure
steps (following \cite{feinberg_chaospy_2015} and many others):
(i) sample from parameter space (experimental design)
(ii) evaluate model at samples
(iii) select an expansion of orthogonal polynomials within the parameters space (up to certain order terms)
(iv) do a regression to calculate the polynomial coefficients
(v) use the model approximation for statistial analysis

% detailed math

vector of random variables
\begin{equation}
    \bm{X} = \{X_1, \dots , X_M\}
\end{equation}

computational model
\begin{equation}
    Y = \cM ( \bm X )
\end{equation}

polynomial chaos expansion
- three terms recurrence algorithm \cite{feinberg_chaospy_2015}
- Hilbertian basis of Hilbert space that contains the response \cite{sudret_global_2008}
\begin{equation}
    Y = \cM ( \bm X ) = \sum_{\ba \,\in\, \mathbb{N}^M} r_{\ba} \psi_{\ba} (\bm X)
\end{equation}
where $\psi_\ba$ is a collection of multivariate orthogonal polynomials that form a Hilbertian basis on $f_{\bm X}$
where $\ba = \{\alpha_1,\dots,\alpha_M\}$ is a multiindex and its elements
denotes the degree of $\psi_\ba$ in each of the $M$ input variables $X_i$
$Y\in\mathbb{R}$ is scalar in the following, but we will look at 8 outputs
$c_\ba \in \mathbb{R}$ are the polynomial coefficients (or PCE coefficients)

orthogonality condition
\begin{equation}
    \langle \psi_{\bm\alpha}, \psi_{\bm\beta} \rangle = \delta_{\bm{\alpha\beta}}
\end{equation}
where $\delta_{\bm{\alpha\beta}}$ is the Kronecker delta which is $1$ if $\bm\alpha=\bm\beta$ and $0$ otherwise.

truncated polynomial chaos expansion
without truncation the polynomial chaos expansion is exact! \cite{fajraoui_optimal_2017}
- standard truncation scheme \cite{gratiet_metamodel-based_2015,sudret_global_2008}
- limit expansion to a number of finite coefficients
- typical degrees 3-5 \cite{gratiet_metamodel-based_2015}
\begin{equation}
    \cM'(\bm X) = \sum_{\ba \,\in\, \cA^{M,p}} r_\ba \psi_\ba(\bm X)
\end{equation}

standard truncation (select all polynomials in $M$ input variables where total degree is less than $p$), it's a set of indices
\begin{equation}
    \cA^{M,p} = \left\{\ba \in \mathbb{N}^M \,:\, \abs{\ba} \leq p\right\}
\end{equation}
where $\abs{\ba} = \sum_{i=1}^M \alpha_i$

cardinality of truncation
\begin{equation}
    P = \card \cA^{M,p} = \left(\begin{matrix}
        M+p \\
        p
    \end{matrix}\right) = \frac{(M+p)!}{M!p!}
\end{equation}
$P$: number of unknown coefficients, also cardinality
$M$: number of uncertain input parameters
$p$: order/degree of the polynomial

coefficients to be found
\begin{equation}
    \bm r = \left\{r_\ba \,:\, \ba \in \cA^{M,p}\right\}
\end{equation}

experimental design
over-sampling ratio (OSR):
- ratio between number of samples and coefficients of polynomial basis (cardinality) \cite{palar_multi-fidelity_2016}
- cardinality increases exponentially with number of input parameters -> curse of dimensionality
- recommended values range between two and three \cite{hosder2007,palar_multi-fidelity_2016,fajraoui_optimal_2017,gratiet_metamodel-based_2015}
- too high OSR: very coarse approximation; too low OSR: risk of over-fitting \cite{palar_multi-fidelity_2016}
OSR considerations (M=5) 2-3
- for order 1, need 12-18 samples
- for order 2, need 42-63 samples
- for order 3, need 112-168 samples
- for order 4, need 252-378 samples
- for order 5, need 504-756 samples
inputs
\begin{equation}
    \mathcal{X} = \set{ \bm x^{(1)},\dots,\bm x^{(N)} }
\end{equation}
outputs
\begin{equation}
    \mathcal{Y} = \set{ \cM\left(\bm x^{(1)}\right),\dots,\cM\left(\bm x^{(N)}\right) }
\end{equation}

regression: minimise least-square residual of polynomial approximation over all samples (Lasso regression: $L_2$ regression with an extra $L_1$ regularization term, and a preference for fewer non-zero terms [sparse].)
\begin{equation}
    \hat{\bm{r}} = \argmin_{\bm{r}_\ba \,\in\, \mathbb{R}^P} \frac{1}{N} \sum_{i=1}^N \left(
        \cM \left(\bm x^{(i)}\right) - \sum_{\ba \,\in\, \cA^{M,p}} r_\ba \psi_\ba\left(\bm x^{(i)}\right)
    \right)^2  + \lambda \norm{\bm{r}_\ba}_1
\end{equation}
$\lambda=0.005$

finished surrogate model
\begin{equation}
    Y = \cM(\bm X) \approx \cM'(\bm X) = \sum_{\ba \,\in\, \cA} \hat{r}_\ba \psi_\ba (\bm X)
\end{equation}

sparsity:
- one can force sparse expansion (few non-zero coefficients) to partially counteract curse of dimensionality \cite{gratiet_metamodel-based_2015}
- these will use regularisation/penalty terms in the formulation of the regression problem
- but ordinary least-square regression is fine if number of uncertain parameters is $\leq 10$ \cite{gratiet_metamodel-based_2015}

\subsubsection{Multifidelity Approach}
\label{sec:multifidelity}

\cite{palar_multi-fidelity_2016}
\cite{ng_multifidelity_2012}
\cite{berchier_multi-fidelity_nodate}

fuse high coverage of parameter space from many low-fidelity samples with
high modelling detail from fewer high-fidelity samples

use a simplified model to sweep the parameter space (many samples, fewer details)
and supplement it with information from a more complex model (fewer samples, more detail)

only high-fidelity for surrogate model is time-consuming,
but only low-fidelity may result in distorted/inaccurate surrogate models

"If LF model is much cheaper to evaluate than the HF model, then significant
computational savings will be obtained" \cite{ng_multifidelity_2012}

integrate both with multi-fidelity approach: correct LF model such that LF output matches HF values

build an own surrogate model for the correction function (i.e. "correction expansion") with identical polynomial basis

concept is "correcting lower order effect of LF expansion with correction function, higher order effects are predicted by LF expansion" \cite{palar_multi-fidelity_2016}

rule: "polynomial term in the correction expansion has to e a subset of the LF expansion" \cite{palar_multi-fidelity_2016}
-> has been checekd, means high-fidelity polynomial order must be lower 

HF sample is subset of LF sample (this is a nice feature of low-discrepancy series)

"in cases with high correlation between LF an HF ($R^2 \geq 0.9$) the MF will very likely increase the approximation accuracy relative to single HF" \cite{palar_multi-fidelity_2016}

method:
- build surrogate A based on low-fidelity samples
- build corrective surrogate B for the difference between high-fidelity samples and predictions of A at those sample points
- add the correction B to the low-fidelity surrogate A to obtain multi-fidelity surrogate C which approximates the high-fidelity model

Additive scaling approach (argue why additive is used not multiplicative)
- Correct for the difference between HF samples and LF surrogate at HF sample points

error function
\begin{equation}
    \mathcal{C}(\bm X) = \cM_h(\bm X) - \cM_\ell(\bm X)
\end{equation}

samples
\begin{align}
    \mathcal{X}_h &= \set{ \bm{x}^{(1)}, \dots, \bm{x}^{(N_h)}} \\
    \mathcal{X}_l &= \set{ \bm{x}^{(1)}, \dots, \bm{x}^{(N_h)}, \dots, \bm{x}^{(N_l)}}
\end{align}
where $N_h < N_\ell$ and when using deterministic low-dicrepancy series
consequently $\mathcal{X}_h \subset \mathcal{X}_\ell$.

Due to the smaller number of samples for the high fidelity model (and therefore error between high and low fidelity), it is also
common that $p_h < p_l$ and consequently $\cA_h \subset \cA_l$

multifidelity model (simple)
\begin{equation}
    \cM_h'(\bm X) = \cM_\ell'(\bm X) + \mathcal{C}'(\bm X)
\end{equation}

multifidelity model (more detailed)
\begin{equation}
    \cM_h' (\bm X) = \sum_{\ba\,\in\,\cA_\ell^{M,p_\ell} \,\cap\, \cA_r^{M,p_c}}
    \left(
     r_{\ell,\ba} + r_{c,\ba}
    \right) \psi_\ba(\bm X) + 
    \sum_{\ba\,\in\,\cA_\ell^{M,p_\ell} \,\setminus\, \cA_r^{M,p_c}}
    r_{\ell,\ba} \psi_\ba(\bm X)
\end{equation}

using chaospy \cite{feinberg_chaospy_2015}

\subsection{Experimental Design}
\label{sec:sampling}

%https://chaospy.readthedocs.io/en/master/sampling/sequences.html
%https://en.wikipedia.org/wiki/Low-discrepancy_sequence
%https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method

sampling / experimental design / collocation points

% explanations
strategies to find sufficiently high coverage of the parameter space at low computational cost \cite{fajraoui_optimal_2017,usher_value_2015}
because traditional MC sampling with pseudo-random numbers has slow convergence % 1 / \sqrt{n}
especially with high-dimensional parameter space - curse of dimensionality challenge
12 in \cite{trondle_trade-offs_2020}, 36 in \cite{pilpola_analyzing_2020}, 5 in \cite{shirizadeh_how_2019} 5 in this paper
low-discrepancy series can improve on random sampling
these deterministic sequences can be used to efficiently sample from the parameter space, as they avoid large gaps and clusters \cite{fajraoui_optimal_2017}
alternatives:
- Latin hypercube sampling \cite{trondle_trade-offs_2020}, but sampling size has to be set a priori, extensions difficult \cite{fajraoui_optimal_2017}
- Method of Morris \cite{usher_value_2015,mavromatidis_uncertainty_2018}
competition/benchmark: \cite{trondle_trade-offs_2020} high fidelity: 10 samples, 400 nodes, 4-hourly; low fidelity: 150 samples, 25 nodes, 4-hourly; no DC power flow

% this paper
consider 5 uniformly distributed uncertain parameters according to \cref{tab:costuncertainty}
500 low fidelity samples
15 high fidelity samples
Halton sequence
chaospy tool \cite{feinberg_chaospy_2015}

how many optimisation runs in this study
- 50000 low-fidelity optimisation runs
- 1000 high-fidelity runs

computational requirements:
- high-fidelity: 20 GB, 5 hours, 4 threads, Gurobi, average values, CPU time ~238 weeks or 4.56 years
- low-fidelity: 3 GB, 5 minutes, 1 thread, Gurobi, average values, CPU time ~25 weeeks or 0.48 years

\subsection{Sensitivity Indices}
\label{sec:sobol}

Sobol / sensitivity indices:
- decomposition of output variance and attribution to random input variables
- separate influential from non-influential parameters
- The polynomials can be used for calculating Sobol indices in post-processing
- orders of magnitude faster than traditional MC evaluation \cite{sudret_global_2008}
- used in \cite{trondle_trade-offs_2020,mavromatidis_uncertainty_2018}

first-order Sobol:
- also main-effect
- share of output variance due to variations in one input parameter alone (averaged over variations in other input parameters)

total Sobol indices:
- contribution to output variance iincluding all interactions with other input variables
- can be greater than 100\% if not purely additive

using chaospy \cite{feinberg_chaospy_2015}

\subsection{Validation and Error Measures}
\label{sec:validation}

error estimation, measures of accuracy

using cross-validation techniques 
- training set of known data
- validation/test set of unknown data to surrogate \cite{gratiet_metamodel-based_2015}
- based on low-fidelity models only
- too few samples for error analysis for high-fidelity model
- out of 500 cost samples, 100 have not been used for building/training the surrogate model; this is our test set.

error measures:
- R2: quality of the regression "proportion of the variance in the dependent variable that is predictable from the independent variable" (wiki)
- MAE: to see absolute deviations (in GW), alternative to relative error (esp if prediction is 0 relative errors are super high)
- MAPE: relative deviations
- RMSE

- benchmark: model cross-validation error below 5\% \cite{trondle_trade-offs_2020}
