In this section,
we first outline how we obtain least-cost and near-optimal solutions for a given cost parameter set.
We then describe the model of the European power system and define the cost uncertainties. 
Finally, we explain how we make use of surrogate modelling techniques
and find an experimental design that efficiently covers the parameter space.

\subsection{Least-Cost Investment Planning}
\label{sec:leastcost}

The objective of long-term power system planning is to minimise the total
annual system costs, comprising annualised capital costs $c_\star$ for investments at locations $i$
in generator capacity $G_{i,r}$ of technology $r$, storage capacity $H_{i,s}$ of technology $s$, and transmission line capacities
$F_{\ell}$, as well as the variable operating costs $o_\star$ for generator dispatch $g_{i,r,t}$:
\begin{align}
    \min_{G,H,F,g} \quad \left\{
        \sum_{i,r}   c_{i,r}  \cdot G_{i,r}  +
        \sum_{i,s}   c_{i,s}  \cdot H_{i,s}  +
        \sum_{\ell}  c_{\ell} \cdot F_{\ell} +
        \sum_{i,r,t} w_t \cdot o_{i,r} \cdot g_{i,r,t}
    \right\}
    \label{eq:objective}
\end{align}
where the snapshots $t$ are weighted by $w_t$ such that their total duration
adds up to one year. The objective is subject to a set of linear constraints that define limits on
(i) the capacities of infrastructure from geographical and technical potentials,
(ii) the availability of variable renewable energy sources for each location and point in time, and
(iii) linearised multi-period optimal power flow (LOPF) constraints including storage consistency equations,
which we describe in more detail in the following.

The capacities of generation, storage and transmission infrastructure are
limited to their geographical potentials from above and existing infrastructure from below:
\begin{align}
    \label{eq:firstA}
    \underline{G}_{i,r}  \leq G_{i,r}  \leq \overline{G}_{i,r}  &\qquad\forall i, r \\
    \underline{H}_{i,s}  \leq H_{i,s}  \leq \overline{H}_{i,s}  &\qquad\forall i, s \\
    \underline{F}_{\ell} \leq F_{\ell} \leq \overline{F}_{\ell} &\qquad\forall \ell
\end{align}

The dispatch of a renewable generator is constrained by
its rated capacity and the time- and location-dependent availability $\overline{g}_{i,r,t}$,
given in per-unit of the generator's capacity:
\begin{align}
    0 \leq g_{i,r,t} \leq \overline{g}_{i,r,t} G_{i,r} \qquad\forall i, r, t
\end{align}
The dispatch of storage units is described by a charge variable $h_{i,s,t}^+$
and a discharge variable $h_{i,s,t}^-$, each limited by the power rating $H_{i,s}$.
\begin{align}
    0 \leq h_{i,s,t}^+ \leq H_{i,s} &\qquad\forall i, s, t \\
    0 \leq h_{i,s,t}^- \leq H_{i,s} &\qquad\forall i, s, t
\end{align}
The energy levels $e_{i,s,t}$ of all storage units are linked to the dispatch by
\begin{align}
    e_{i,s,t} =\: & \eta_{i,s,0}^{w_t} \cdot e_{i,s,t-1} + w_t \cdot h_{i,s,t}^\text{inflow} - w_t \cdot h_{i,s,t}^\text{spillage} & \quad\forall i, s, t \nonumber \\
    & + \eta_{i,s,+} \cdot w_t \cdot h_{i,s,t}^+ - \eta_{i,s,-}^{-1} \cdot w_t \cdot h_{i,s,t}^-.
\end{align}
Storage units can have a standing loss $\eta_{i,s,0}$, a charging efficiency $\eta_{i,s,+}$, a discharging efficiency $\eta_{i,s,-}$,
natural inflow $h_{i,s,t}^\text{inflow}$ and spillage $h_{i,s,t}^\text{spillage}$.
The storage energy levels are assumed to be cyclic and are constrained by their energy capacity
\begin{align}
    e_{i,s,0} = e_{i,s,T} &\qquad\forall i, s \\
    0 \leq e_{i,s,t} \leq \overline{T}_s \cdot H_{i,s} &\qquad\forall i, s, t.
\end{align}
To reduce the number of decisison variables, we link the energy capacity to
power ratings with a technology-specific parameter $\overline{T}_s$
that describes the maximum duration a storage unit can discharge at full power rating.

Kirchhoff's Current Law (KCL) requires local generators and storage units as well as
incoming or outgoing flows $f_{\ell,t}$ of incident transmission lines $\ell$
to balance the inelastic electricity demand $d_{i,t}$ at each location $i$ and snapshot $t$
\begin{align}
    \sum_r g_{i,r,t} + \sum_s h_{i,s,t} + \sum_\ell K_{i\ell} f_{\ell,t} = d_{i,t} \qquad\forall i,t,
\end{align}
where $K_{i\ell}$ is the incidence matrix of the network.

Kichhoff's Voltage Law (KVL) imposes further constraints on the flow of AC lines.
Using linearised load flow assumptions, the voltage angle difference around every closed cycle in the
network must add up to zero. We formulate this constraint using a cycle basis $C_{\ell c}$
of the network graph where the independent cycles $c$ are expressed as
directed linear combinations of lines $\ell$ \cite{cycleflows}.
This leads to the constraints
\begin{align}
    \sum_\ell C_{\ell c} \cdot x_\ell \cdot f_{\ell,t} = 0 \qquad\forall c,t
    \label{eq:kvl}
\end{align}
where $x_\ell$ is the series inductive reactance of line $\ell$.
Controllable HVDC links are not affected by this constraint.

Finally, all line flows $f_{\ell,t}$ must be operated within their nominal capacities $F_\ell$
\begin{align}
    \abs{f_{\ell,t}} \leq \overline{f}_{\ell} F_{\ell} & \qquad\forall \ell, t,
    \label{eq:lastA}
\end{align}
where $\overline{f}_\ell$ acts as a per-unit buffer capacity
to protect against the outage of single circuits.

This problem is implemented in the open-source tool PyPSA \cite{pypsa} and is solved by Gurobi.
Note, that it assumes perfect foresight for a single reference year based on which capacities are optimised.
It does not include pathway optimisation, nor aspects of reserve power, or system stability.
Changes of line expansion to line impedance are ignored.

\subsection{Near-Optimal Alternatives}
\label{sec:nearoptimal}

Using the least-cost solution as an anchor, we use the
$\varepsilon$-constraint method from multi-objective optimisation
to find near-optimal feasible solutions \cite{nearoptimal,mavrotas_effective_2009}.
For notational brevity, let $c^\top x$ denote the linear objective function \cref{eq:objective}
and $Ax\leq b$ the set of linear constraints \crefrange{eq:firstA}{eq:lastA}
in a space of continuous variables, 
such that the minimised system cost can be represented by
\begin{equation}
    C = \min_x\left\{c^\top x \mid Ax\leq b\right\}.
\end{equation}

% one technology

We then encode the original objective as a constraint 
such that the cost increase is limited to a given $\varepsilon$.
In other words, the feasible space is cut to solutions that
are at most $\varepsilon$ more expensive than the least-cost solution.
Given this slack, we can formulate alternative search directions in the objective.
For instance, we can seek to minimise the sum of solar installations $x_s \subseteq x$ with
\begin{equation}
    \overline{x_s} = \min_{x_s}\left\{\: 1^\top x_s \mid Ax\leq b,\quad c^\top x\leq (1+\varepsilon)\cdot C \:\right\}.
\end{equation}
To draw a full picture of the frontiers of near-optimal feasible space, 
we systematically explore the extremes of various technologies:
we both minimise and maximise the system-wide investments in
solar, onshore wind, offshore wind, any wind, hydrogen storage, and battery storage
capacities, as well as the total volume of transmission network expansion.
Evaluating each of these technology groups for
different cost deviations $\varepsilon \in \{1\%,2\%,4\%,6\%,8\%\}$
allows us to observe how the degree of freedom regarding investment decisions
rises as the optimality tolerance is increased, both at lower and upper ends.
By arguments of convexity, these extremes even delineate boundaries
within which all near-optimal solutions are contained.
Moreover, although this scheme primarily studies aggregated capacities,
the solutions are still spatially explicit, and we can inspect for each case
how the capacities of each technology are distributed in the network.

% two technologies

A caveat of the method hides in the fact that the extremes
of different technologies cannot be realised simultaneously.
This is because technologies that are not part of the active search direction
will act to support pushing the technologies in the objective further.
For instance, lowering wind capacities will be compensated for
by higher solar capacities to be able to satisfy energy demand.
Thus, extremising one technology narrows the flexibility of a substitute technology. The question about the nature of such dependencies arises.
To investigate two-dimensional influences,
in addition to the $\varepsilon$-constraint and the objective to
extremise use of a particular technology,
we formulate a constraint that fixes the capacity of another technology.
An example would be to search for the minimum amount of wind capacity $x_w \subseteq x$ 
given that a certain amount of solar is built
\begin{equation}
    \overline{x_w} = \min_{x_w}\left\{\:1^\top x_w \mid Ax\leq b,\quad c^\top x\leq (1+\varepsilon)\cdot C, \quad 1^\top x_s = \underline{x_s} + \alpha \cdot (\overline{x_s}-\underline{x_s}) \:\right\}.
\end{equation}
The $\alpha$ denotes the relative position within the near-optimal
range of solar capacities at given $\varepsilon$.
For example, at $\alpha=0\%$ we look for the least wind capacity
given that minimal solar capacities are built. 
An alternative but more complex approach to spanning the space of near-optimal solutions in multiple dimensions at a time
using a quick hull algorithm was presented by Pedersen et al.~\cite{pedersen_modeling_2020}.

Due to computational constraints, we focus on technologies which
are assumed to lend themselves to substitution and limit the analysis to a single
cost increase level of $\varepsilon=6\%$.
We consider the three pairs,
(i) wind and solar,
(ii) offshore and onshore wind,
(iii) hydrogen and battery storage,
by minimising and maximising the former while
fixing the latter at positions $\alpha \in \{0\%,25\%,50\%,75\%,100\%\}$
within the respective near-optimal range.

\subsection{Model Inputs}
\label{sec:inputs}

\begin{SCfigure}
    \begin{tabular}{cc}
        \footnotesize (a) low-fidelity: 37 nodes and 4-hourly & \footnotesize (b) high-fidelity: 128 nodes and 2-hourly \\
        \includegraphics[width=0.33\textwidth]{map37.pdf} &
        \includegraphics[width=0.33\textwidth]{map128.pdf} \\
        \includegraphics[width=0.33\textwidth]{timeseries37.pdf} &
        \includegraphics[width=0.33\textwidth]{timeseries128.pdf} \\
    \end{tabular}
    \caption{Illustration of the spatial and temporal resolution of the low and high fidelity model.}
    \label{fig:pypsaeur}
\end{SCfigure}

% general

The instances of the coordinated capacity expansion problem (\cref{sec:leastcost})
 are based on \mbox{PyPSA-Eur},
which is an open model of the European power transmission system
that combines high spatial and temporal resolution \cite{pypsaeur}.
Its sole reliance on open data from a variety of sources
and formulation as an open-source snakemake workflow
that tracks computations from raw data to the solved model,
makes the analysis conducted in this study transparent and reproducible \cite{snakemake}.
In the following, we outline the main features and chosen configurations. 
However, refer to the supplementary material and \cite{pypsaeur} for more extensive details.

We target a fully renewable electricity system primarily based on variable resources
such as solar photovoltaics, onshore wind and offshore wind.
Predominantly we pursue a greenfield approach, with a few notable exceptions.
The existing hydro-electric infrastructure (run-of-river, hydro dams, pumped-storage)
is included but not considered to be extendable due to assumed geographical constraints.
Furthermore, the existing transmission infrastructure can only
be reinforced continuously but may not be removed.
In addition to balancing renewables in space with transmission networks,
the model includes storage options to balance renewables in time.
We consider two extendable storage technologies:
battery storage representing short-term storage suited to balancing daily fluctuations and
hydrogen storage system which exemplifies long-term synoptic and seasonal storage.

Since the spatial and temporal resolution strongly affects the size of the optimisation problem,
running the model at full resolution is computationally infeasible.
Throughout the paper, we will therefore make use of two levels of aggregation,
reflecting a compromise between the computational burden incurred by high-resolution models and
the growing inaccuracies regarding transmission bottlenecks
and resource distribution in low-resolution models.
We consider a low fidelity model with 37 nodes at a 4-hourly resolution
that models power flow via a transport model (i.e.~excluding KVL of \cref{eq:kvl}) and
a high fidelity model with 128 nodes at a 2-hourly resolution
subject to linearised load flow constraints.

% existing infrastructure (hydro and transmission)

The topology of the European transmission network is retrieved
from the ENTSO-E transparency map and includes all lines at and above 220 kV.
Capacities and electrical characteristics of transmission lines and substations are
inferred from standard types for each voltage level, before they are transformed to
a uniform voltage level.
For each line, $N-1$ security is approximated by limiting the line loading to 70\% of its nominal rating.
The dataset further includes existing HVDC links and planned projects from the TYNDP.
Existing run-of-river, hydro-electric dams, pumped-hydro storage plants
are retrieved from powerplantmatching, a merged dataset of conventional power plants.

% renewable generation potentials

Eligible areas for developing renewable infrastructure are calculated
per technology and grid node, assuming wind and solar installations always connect to the closest substation. 
How much wind and solar capacity may be built at a particular location is constrained by selected
eligible codes of the CORINE land use database and is further restricted by distance criteria,
allowed deployment density, and the natural protection areas specified in the NATURA 2000 dataset.
Offshore wind farms may not be developed at sea depths exceeding 50 metres,
as indicated by the GEBCO bathymetry dataset. 

% renewable generation and demand time series

The location-dependent renewables availability time series are generated
based on two historical weather datasets from the year 2013.
We retrieve wind speeds, run-off and surface roughness from the ERA5 reanalysis dataset and
use the satellite-aided SARAH-2 dataset for the direct and diffuse surface solar irradiance.
Models for wind turbines, solar panels, and the inflow into the basins of hydro-electric dams
convert the weather data to hourly capacity factors and aggregate these to each grid node.
Country-level load time series are taken from ENTSO-E statistics and
are distributed to each grid node to 40\% by population density and to 60\% by gross domestic product.

\subsection{Technology Cost Uncertainty}
\label{sec:uncertainty}
%\blindtext

\begin{SCtable}
    \begin{small}
        \begin{tabular}{cccc}
            \toprule
            Technology & Lower Annuity & Upper Annuity & Unit  \\ \midrule
            Onshore Wind & 73 & 109 & EUR/kW/a \\
            Offshore Wind & 178 & 245 & EUR/kW/a \\ % this includes connection cost!
            Solar & 36 & 53 & EUR/kW/a \\
            Battery & 30 & 125 & EUR/kW/a \\
            Hydrogen & 111 & 259 & EUR/kW/a \\ \bottomrule
        \end{tabular}
    \end{small}
    \caption{Technology cost uncertainty using optimistic and pessimistic assumptions from DEA.}
    \label{tab:costuncertainty}
\end{SCtable}   

% General

The uncertainty of technology cost projections is driven 
by unknown learning rates (i.e.~how quickly costs fall as more capacity is built)
as well as unclear deployment rates (i.e.~how much capacity will be built in the future) \cite{gritsevskyi_modeling_2000,yeh_review_2012}.
As modelling technological learning endogeneously is computationally challenging \cite{heuberger_power_2017,mattsson_learning_2019},
% While uncertainty about the latter could generally be addressed by modelling endogeneous
% learning in an approach that includes multiple investment periods,
% the exponential influence of the learning rate raises the computational complexity immensely
technology cost uncertainty is typically defined exogenously by an interval within costs may vary
and a distribution that specifies which segments are more probable.

% Ranges

Ranges of cost projections are best chosen as wide as possible to avoid excluding any plausible scenarios \cite{moret_characterization_2017,mccollum_energy_2020}.
Commonly, have been varying between $\pm20\%$ and $\pm65\%$ depending on maturity of the technology \cite{moret_characterization_2017,shirizadeh_how_2019,pizarro-alonso_uncertainties_2019,li_using_2020,trondle_trade-offs_2020}.
In this study, we consider uncertainty of the annuities of
onshore wind, offshore wind, solar PV, battery and hydrogen storage systems.
In this context, hydrogen storage systems comprise the cost of
electrolysis, cavern storage, and fuel cells.
For solar PV we assume an even split between utility-scale PV and residential rooftop PV.
Evaluating uncertainties based on annuities has a distinct advantage.
They can be seen to simultaneously incorporate uncertainties about
the overnight investments, fixed operation and maintenance costs,
their lifetime, and the discount rate,
since multiple parameter combinations lead to the same annuity.
We built the uncertainty ranges presented in \cref{tab:costuncertainty}
from the optimistic and pessimistic technology cost
and lifetime projections for the year 2050 from the Danish Energy Agency,
which correspond to 90\% confidence intervals \cite{DEA}.
We further include projections for the year 2030 when they can further expand the interval
or in case no ranges were provided for the year 2050, such as for rooftop PV.

% Distributions

Distributions of cost projections have been guessed
to follow normal \cite{mavromatidis_uncertainty_2018} or triangular distributions \cite{li_using_2020}.
But independent uniform distributions are the most prevalent assumption \cite{moret_characterization_2017,moret_robust_2016,shirizadeh_how_2019,trondle_trade-offs_2020,pilpola_analyzing_2020,Li2017,Trutnevyte2013,lopion_cost_2019}.
This approach is backed by the maximum entropy approach \cite{trondle_trade-offs_2020}, which states that given the lack of knowledge about the distribution
the independent uniform distribution that makes fewest assumptions is most appropriate.
Although the assumed independence may neglect synergies between technologies,
for example, between offshore and onshore wind turbine development,
we follow the literature by assuming that the cost are independent and uniformly distributed within the ranges specified in \cref{tab:costuncertainty}.

\subsection{Surrogate Modelling}
\label{sec:surrogate}
%\blindtext

% problem

Searching for least-cost solutions (\cref{sec:leastcost}) and many associated near-optimal alternatives (\cref{sec:nearoptimal}) of a highly resolved power system model (\cref{sec:inputs}) on its own
is already labour-intensive from a computational perspective.
Repeating this search for a large variety of cost assumptions (\cref{sec:uncertainty}), 
to be able to make statements about the robustness of
investment flexibility near the optimum under uncertainty,
adds another layer to the computational burden.

% idea of surrogate models

Surrogate models\footnote{Surrogate names are also known by names such as approximation models, response surface methods, metamodels and emulators.}
offer a solution for such cases, where the outcome of the
original model cannot be obtained without great effort.
In contrast to the full model, they only imitate the input/output behaviour for a choice of aggregated outputs, but take much less time to compute \cite{palar_multi-fidelity_2016}.
Like other machine learning techniques, they can generalise from a
training dataset that comprises only a limited number of samples.
As surrogate models interpolate gaps in the parameter space that are not contained in the sample set,
that would otherwise be computationally expensive to fill,
they are well suited to use cases such as parameter space exploration and sensitivity analysis.

% this paper

In this paper, we will make use of surrogate models that map the
cost of onshore wind, offshore wind, solar, hydrogen, and battery storage (\cref{tab:costuncertainty})
onto a selection of eight system-level outputs.
These are the total system cost and the installed onshore wind, offshore wind, solar, hydrogen, battery, and transmission network capacities.
We construct surrogate models for least-cost and near-optimal solutions separately
for each system cost slack, search direction, fixed total capacity, and output variable. This results in a collection of 801 individual surrogate models, one for each optimisation problem that is solved
for a given set of cost assumptions.
The method we choose from an abundance of alternatives is based on polynomial chaos expansion (PCE)
\cite{sudret_global_2008,fajraoui_optimal_2017,gratiet_metamodel-based_2015}.
We select this approach because the resulting approximations
allow efficient analytical statistical evaluation \cite{sudret_global_2008} and
can conveniently combine training data from variously detailed models \cite{palar_multi-fidelity_2016}.

\subsubsection{Use of Polynomial Chaos Expansion}
\label{sec:pce}

% general idea

The general idea of surrogate models based on PCE is to
represent uncertain model outputs as a linear combination of orthogonal basis functions 
of the random input variables weighted by deterministic coefficients \cite{muhlpfordt_uncertainty_2020}.
It is a Hilbert space technique that works analogously to decomposing a periodic signal into its Fourier components \cite{muhlpfordt_uncertainty_2020}.
Building the surrogate model consists of the following steps:
(i) sampling a set of cost projections from the parameter space,
(ii) solving the least-cost or near-optimal investment planning problem for each sample,
(iii) selecting an expansion of orthogonal polynomials within the parameter space,
(iv) performing a regression to calculate the polynomial coefficients, and ultimately
(v) using the model approximation for statistical analysis.
In the following, we will formalise this approach,
which we implemented using the \textit{chaospy} toolbox \cite{feinberg_chaospy_2015},
and elaborate on individual aspects in more detail.

We start by defining the vector of random input variables as
\begin{equation}
    \x = \{\mathsf{x}_1, \dots , \mathsf{x}_m\}
\end{equation}
that represents the $m$ uncertain cost projections.
Further, we let
\begin{equation}
    \mathsf{y} = f(\x)
\end{equation}
describe how the uncertainty of inputs $\x$ propagates
through the computationally intensive model $f$
(i.e.~the solving a large optimisation problem)
to the outputs $\mathsf{y} \in \mathbb{R}$.
% For simplicity, we limit the subsequent explanation to a scalar output,
% but it is straightforward to expand the theory to vectorised outputs.

We can represent the computational model $f$ with its polynomial chaos expansion
\begin{equation}
    \mathsf{y} = f(\x) = \sum_{\ba \,\in\, \mathbb{N}^m} r_{\ba} \psi_{\ba}(\x)
    \label{eq:pce}
\end{equation}
where $\psi_\ba$ denotes multivariate orthogonal polynomials that form
a Hilbertian basis and $r_\ba \in \mathbb{R}$ are the corresponding polynomial coefficients \cite{sudret_global_2008}. The multiindex $\ba = \{\alpha_1,\dots,\alpha_m\}$ 
denotes the degree of the polynomial $\psi_\ba$ in each of the $m$ random input variables $\mathsf{x}_i$.
As \cref{eq:pce} features an infinite number of unknown coefficients,
it is common practice to approximate by truncating the expansion to get a finite number of coefficients
\begin{equation}
    f(\x) \approx f'(\x) = \sum_{\ba \,\in\, \cA^{m,p}} r_\ba \psi_\ba(\x).
\end{equation}
In the standard truncation scheme \cite{gratiet_metamodel-based_2015,sudret_global_2008},
all polynomials in $m$ input variables where the total degree is less than $p$ are selected.
We can write this as a set of indices
\begin{equation}
    \cA^{m,p} = \left\{\ba \in \mathbb{N}^m \,:\, \abs{\ba} \leq p\right\},
\end{equation}
where $\abs{\ba} = \sum_{i=1}^m \alpha_i$.
Given the joint distribution of $\x$ and a maximum degree,
a suitable collection of orthogonal polynomials can be constructed
using a three terms recurrence algorithm \cite{feinberg_chaospy_2015}.
The cardinality of the truncated PCE,
\begin{equation}
    q = \card \cA^{m,p} = \left(\begin{matrix}
        m+p \\
        p
    \end{matrix}\right) = \frac{(m+p)!}{m!p!},
\end{equation}
indicates the number of unknown polynomial coefficients.

We determine these coefficicients by a regression based on
a set of cost parameter samples
and the corresponding outputs,
\begin{equation}
    \mathcal{X} = \set{ \bm x^{(1)},\dots,\bm x^{(n)} } \quad\text{and}\quad 
    \mathcal{Y} = \set{ f\left(\bm x^{(1)}\right),\dots,f\left(\bm x^{(n)}\right) }.
\end{equation}
Using this training dataset, we minimise the least-square residual of the polynomial approximation across all observations.
Following the LASSO approach, we add an extra $L_1$ regularisation term,
that induces a preference for fewer non-zero coefficients, and solve
\begin{equation}
    \hat{\bm{r}} = \argmin_{\bm{r} \,\in\, \mathbb{R}^q} \left[ \frac{1}{n} \sum_{i=1}^n \left(
        f\left(\bm x^{(i)}\right) - \sum_{\ba \,\in\, \cA^{m,p}} r_\ba \psi_\ba\left(\bm x^{(i)}\right)
        \right)^2  + \lambda \norm{\bm{r}}_1 \right],
        \label{eq:regression}
    \end{equation}
where we set the regularisation penalty to $\lambda=0.005$.
This results in a sparse PCE that has proven to
improve approximations in high-dimensional uncertainty spaces
and reduce the required number of samples for comparable approximation errors \cite{gratiet_metamodel-based_2015}.
Knowing the optimised regression coefficients, we can assemble the complete surrogate model
\begin{equation}
    \mathsf{y} = f(\x) \approx f'(\x) = \sum_{\ba \,\in\, \cA^{m,p}} \hat{r}_\ba \psi_\ba (\x).
\end{equation}

\subsubsection{Extension by Multifidelity Approach}
\label{sec:multifidelity}

% idea

To construct a precise PCE-based surrogate model, it is desirable to base it
on many samples from a high-fidelity model.
However, this is likely prohibitively time-consuming.
On the other hand, relying only on samples from a
low-fidelity model may be too inaccurate \cite{ng_multifidelity_2012}.
For example, an investment model that features only a single node per country
will underestimate transmission bottlenecks and regionally uneven resource or demand distribution.
In \cref{sec:inputs} we already alluded to using
two models with varying spatial and temporal resolution in this paper.
We integrate both in a multi-fidelity approach \cite{ng_multifidelity_2012,palar_multi-fidelity_2016}.
We demonstrate how we can
simultaneously avail of high coverage of the parameter space by sampling
the simpler model many times, and the high spatio-temporal detail yielded by fewer more complex model runs.

% method

The idea of the multi-fidelity approach is to build a corrective surrogate model $\Delta'(\x)$ for the
error of the low-fidelity model $f_\ell$ compared to the high-fidelity model $f_h$ 
\begin{equation}
    \Delta(\x) = f_h(\x) - f_\ell(\x),
\end{equation}
and add it to a surrogate model of the low-fidelity model to approximate the behaviour of the
high-fidelity model
\begin{equation}
    f_h'(\x) = f_\ell'(\x) + \Delta'(\x).
\end{equation}
Typically, the corrective PCE only rectifies only the lower order effects
of the low-fidelity surrogate model \cite{palar_multi-fidelity_2016}.
The advantage is that this way the correction function can be determined
based on fewer samples analogous to \cref{sec:pce}.
To sample the errors, it is only required that the
high-fidelity samples are a subset of the low-fidelity samples, e.g.
\begin{equation}
    \mathcal{X}_h = \set{ \bm{x}^{(1)}, \dots, \bm{x}^{(n_h)}} \quad\text{and}\quad
    \mathcal{X}_\ell = \set{ \bm{x}^{(1)}, \dots, \bm{x}^{(n_h)}, \dots, \bm{x}^{(n_\ell)}},
\end{equation}
which we can guarantee by using deterministic low-discrepancy series
in the experimental design (\cref{sec:sampling}).
With $p_c < p_\ell$ and consequently $\cA_h \subset \cA_\ell$,
the multi-fidelity surrogate model can be written as a combination of low-fidelity
and corrective polynomial coefficients
\begin{equation}
    f_h' (\x) = \sum_{\ba\,\in\,\cA_\ell^{m,p_\ell} \,\cap\, \cA_r^{m,p_c}}
    \left(
     r_{\ell,\ba} + r_{c,\ba}
    \right) \psi_\ba(\x) + 
    \sum_{\ba\,\in\,\cA_\ell^{m,p_\ell} \,\setminus\, \cA_r^{m,p_c}}
    r_{\ell,\ba} \psi_\ba(\x).
\end{equation}

% this paper

This work applies a multi-fidelity surrogate model that considers
effects up to order three observed in the low-fidelity model. These are corrected with linear terms derived from insights from the high-fidelity model.
We justify this choice by experimentation in \cref{sec:validation}.
Given the polynomial expansion order, the remaining question is how many samples are necessary to attain an acceptable approximation.

\subsection{Experimental Design}
\label{sec:sampling}

% explanations

final aspect: generating enough samples based on which approximation model is built
sampling / experimental design / collocation points in the uncertainty space
strategies to find sufficiently high coverage of the parameter space at low computational cost \cite{fajraoui_optimal_2017,usher_value_2015}
because traditional MC sampling with pseudo-random numbers has slow convergence % 1 / \sqrt{n}
especially with high-dimensional parameter space - curse of dimensionality challenge
12 in \cite{trondle_trade-offs_2020}, 36 in \cite{pilpola_analyzing_2020}, 5 in \cite{shirizadeh_how_2019}
low-discrepancy series can improve on random sampling
these deterministic sequences can be used to efficiently sample from the parameter space, as they avoid large gaps and clusters \cite{fajraoui_optimal_2017}
alternatives:
- Latin hypercube sampling \cite{trondle_trade-offs_2020}, but sampling size has to be set a priori, extensions difficult \cite{fajraoui_optimal_2017}
- Method of Morris \cite{usher_value_2015,mavromatidis_uncertainty_2018}
competition/benchmark: \cite{trondle_trade-offs_2020}
- high fidelity: 10 samples, 400 nodes, 4-hourly;
- low fidelity: 150 samples, 25 nodes, 4-hourly; no DC power flow
resort to oversampling ratio as guideline:
It is defined as the ratio between the number of samples
and the number of coefficients of polynomial basis \cite{palar_multi-fidelity_2016}.
- too high: very coarse approximation; too low: risk of over-fitting \cite{palar_multi-fidelity_2016}
- recommended values range between two and three \cite{hosder2007,palar_multi-fidelity_2016,fajraoui_optimal_2017,gratiet_metamodel-based_2015}
- typical degrees 3-5 \cite{gratiet_metamodel-based_2015}
- cardinality increases exponentially with number of input parameters
  - m=5, p=1: 18
  - m=5, p=2: 63
  - m=5, p=3: 168
  - m=5, p=4: 378
  - m=5, p=5: 756

% this paper

consider the 5 uniformly distributed independent uncertain parameters according to \cref{tab:costuncertainty}
500 low-fidelity samples (3:OSR=8.9, 5:OSR=2.0)
15 high-fidelity samples (OSR=2.5)
Halton sequence
% actually fewer would have been sufficient for the chosen polynomial orders,
% but this number necessary to investigate the trade-off between higher order polynomials and fewer required samples

how many optimisation runs in this study
- all combinations of search directions, slacks, samples presented in preceding sections lead to
- 50000 low-fidelity optimisation runs
- 1000 high-fidelity runs

computational requirements:
- high-fidelity: 20 GB, 5 hours, 4 threads, Gurobi, average 
- low-fidelity: 3 GB, 5 minutes, 1 thread, Gurobi, average
- while optimisation generally no boost from paralelisation
- generation of near-optimal solution profits tremendously from paralelisation
- only possible on high performance computing infrastructure

\subsection{Model Validation}
\label{sec:validation}
%%\blindtext

\begin{figure}
    \noindent\makebox[\textwidth]{
        \begin{subfigure}[t]{1.4\textwidth}
            \caption{number of samples}
            \label{fig:error:samples}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-r2-vs-samples-order-3-sklearn.pdf}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-mape-vs-samples-order-3-sklearn.pdf}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-mae-vs-samples-order-3-sklearn.pdf}
            \includegraphics[height=.2\textwidth]{error/error-rmse-vs-samples-order-3-sklearn.pdf}
        \end{subfigure}
    }
    \noindent\makebox[\textwidth]{
        \begin{subfigure}[t]{1.4\textwidth}
            \caption{polynomial order}
            \label{fig:error:poly}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-r2-vs-order-sklearn.pdf}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-mape-vs-order-sklearn.pdf}
            \includegraphics[height=.2\textwidth, trim=0cm 0cm 4.5cm 0cm, clip]{error/error-mae-vs-order-sklearn.pdf}
            \includegraphics[height=.2\textwidth]{error/error-rmse-vs-order-sklearn.pdf}
        \end{subfigure}
    }
    \caption{Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua.}
    \label{fig:error}
\end{figure}

evaluate combinations of order and sample size experimentally in \cref{sec:validation}

using cross-validation techniques 
- training set of known data
- validation/test set of unknown data to surrogate \cite{gratiet_metamodel-based_2015}
- based on low-fidelity models least-cost solutions only
    - too few samples for error analysis for high-fidelity model
    - no fundamental deviations exptected for near-optimal solutions
- out of 500 cost samples, 100 have not been used for building/training the surrogate model; this is our test set.

error estimation, measures of accuracy:
- R2: extent of variance captured by regression quality
- MAE: to see absolute deviations
- MAPE: relative deviations
- RMSE:
- benchmark: model cross-validation error below 5\% \cite{trondle_trade-offs_2020}

\cref{fig:error}
- \cref{fig:error:samples}
  - thanks to the regularisation term, already acceptable results with as little as 50 samples
  - no significant approvement after 150 samples
  - achieve average relative errors of less than 4\%
  - (exceptions are offshore wind and battery for which the relative measure is distorted because frequently these technologies are not used at all)
  - overall the prediction of tsc is remarkably excellent (negligible errors)
  - determination coefficients above 0.95 showing that the surrogate model captures the output variance very well
  - (exception is transmission -- why?)
- \cref{fig:error:poly}
  - order of 2 and below is too simple (underfitting)
  - order of 4 and above yields no improvement (overfitting, were it not for the regularisation term)
  - as higher order requires more samples, in the interest of computational burden, order of 3 appears to be a suitable compromise